from os.path import join
import os
from tracemalloc import start
import torch
import numpy as np
from NetWorks.HeadNeRFNet import HeadNeRFNet
import cv2
from HeadNeRFOptions import BaseOptions
from Utils.HeadNeRFLossUtils import HeadNeRFLossUtils
from Utils.RenderUtils import RenderUtils
import pickle as pkl
import time
from glob import glob
from tqdm import tqdm
import imageio
import random
import argparse
from tool_funcs import put_text_alignmentcenter
import h5py

#define eye gaze base
UPPER_RIGHT = torch.tensor([1,-1])
UPPER_LEFT = torch.tensor([1,1])
LOWER_RIGHT = torch.tensor([-1,-1])
LOWER_LEFT = torch.tensor([-1,1])

def gaze_feat_tensor(gaze_dim,scale_factor,base_gaze_value):
    return base_gaze_value.repeat(1,gaze_dim//base_gaze_value.size(0)) * scale_factor


class FittingImage(object):
    
    def __init__(self, model_path, save_root, gpu_id, include_eye_gaze=True, eye_gaze_dim=16,gaze_scale_factor=1,vis_vect=False) -> None:
        super().__init__()
        self.model_path = model_path

        self.device = torch.device("cuda:%d" % gpu_id)
        torch.cuda.set_device(self.device)
        self.save_root = save_root
        self.opt_cam = True
        self.view_num = 45
        self.duration = 3.0 / self.view_num
        self.model_name = os.path.basename(model_path)[:-4]
        self.include_eye_gaze = include_eye_gaze
        self.eye_gaze_dim = eye_gaze_dim
        self.scale_factor = gaze_scale_factor
        self.vis_vect = vis_vect
        

        self.build_info()
        self.build_tool_funcs()


    def build_info(self):
        check_dict = torch.load(self.model_path, map_location=torch.device("cpu"))

        para_dict = check_dict["para"]
        self.opt = BaseOptions(para_dict) #just use the same feature size as the para_dict

        self.featmap_size = self.opt.featmap_size
        self.pred_img_size = self.opt.pred_img_size
        
        if not os.path.exists(self.save_root): os.mkdir(self.save_root)

        net = HeadNeRFNet(self.opt, include_vd=False, hier_sampling=False, include_gaze=self.include_eye_gaze,eye_gaze_dim=self.eye_gaze_dim)        
        net.load_state_dict(check_dict["net"])
        
        self.net = net.to(self.device)
        self.net.eval()


    def build_tool_funcs(self):
        self.loss_utils = HeadNeRFLossUtils(device=self.device)
        self.render_utils = RenderUtils(view_num=45, device=self.device, opt=self.opt)
        
        self.xy = self.render_utils.ray_xy
        self.uv = self.render_utils.ray_uv
    

    def load_data(self,hdf_path,img_index):
        #process imgs
        self.hdf = h5py.File(hdf_path, 'r', swmr=True)
        assert self.hdf.swmr_mode

        img_size = (self.pred_img_size, self.pred_img_size)

        img = self.hdf['face_patch'][img_index]
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = img.astype(np.float32)/255.0
        
        gt_img_size = img.shape[0]
        if gt_img_size != self.pred_img_size:
            img = cv2.resize(img, dsize=img_size, fx=0, fy=0, interpolation=cv2.INTER_LINEAR)
        
        mask_img =  self.hdf['mask'][img_index]
        if mask_img.shape[0] != self.pred_img_size:
            mask_img = cv2.resize(mask_img, dsize=img_size, fx=0, fy=0, interpolation=cv2.INTER_NEAREST)
        img[mask_img < 0.5] = 1.0
        
        self.img_tensor = (torch.from_numpy(img).permute(2, 0, 1)).unsqueeze(0).to(self.device)
        self.mask_tensor = torch.from_numpy(mask_img[None, :, :]).unsqueeze(0).to(self.device)
        

        gaze_label = self.hdf['face_gaze'][img_index]
        gaze_label = gaze_label.astype('float')
        gaze_tensor = (torch.from_numpy(gaze_label)).to(self.device)
        self.base_gaze = gaze_tensor.repeat(1,self.eye_gaze_dim//gaze_tensor.size(0))
        

       # load init codes from the results generated by solving 3DMM rendering opt.
        nl3dmm_para_dict = self.hdf['nl3dmm']
        base_code = torch.from_numpy(nl3dmm_para_dict["code"][img_index]).float().detach().unsqueeze(0).to(self.device)
        
        #ablation on 3DMM model codes
        IGNORE_3DMM_CODE=False
        if IGNORE_3DMM_CODE:
            base_code_zero = torch.zeros_like(base_code)
            base_code = base_code_zero
        
        self.base_iden = base_code[:, :self.opt.iden_code_dims]
        self.base_expr = base_code[:, self.opt.iden_code_dims:self.opt.iden_code_dims + self.opt.expr_code_dims]
        self.base_text = base_code[:, self.opt.iden_code_dims + self.opt.expr_code_dims:self.opt.iden_code_dims 
                                                            + self.opt.expr_code_dims + self.opt.text_code_dims]
        self.base_illu = base_code[:, self.opt.iden_code_dims + self.opt.expr_code_dims + self.opt.text_code_dims:]
        
        self.base_c2w_Rmat = torch.from_numpy(nl3dmm_para_dict["c2w_Rmat"][img_index]).float().detach().unsqueeze(0)
        self.base_c2w_Tvec = torch.from_numpy(nl3dmm_para_dict["c2w_Tvec"][img_index]).float().detach().unsqueeze(0).unsqueeze(-1)
        self.base_w2c_Rmat = torch.from_numpy(nl3dmm_para_dict["w2c_Rmat"][img_index]).float().detach().unsqueeze(0)
        self.base_w2c_Tvec = torch.from_numpy(nl3dmm_para_dict["w2c_Tvec"][img_index]).float().detach().unsqueeze(0).unsqueeze(-1)

        temp_inmat = torch.from_numpy(nl3dmm_para_dict["inmat"][img_index]).detach().unsqueeze(0)
        temp_inmat[:, :2, :] *= (self.featmap_size / gt_img_size)
        
        temp_inv_inmat = torch.zeros_like(temp_inmat)
        temp_inv_inmat[:, 0, 0] = 1.0 / temp_inmat[:, 0, 0]
        temp_inv_inmat[:, 1, 1] = 1.0 / temp_inmat[:, 1, 1]
        temp_inv_inmat[:, 0, 2] = -(temp_inmat[:, 0, 2] / temp_inmat[:, 0, 0])
        temp_inv_inmat[:, 1, 2] = -(temp_inmat[:, 1, 2] / temp_inmat[:, 1, 1])
        temp_inv_inmat[:, 2, 2] = 1.0
        
        self.temp_inmat = temp_inmat
        self.temp_inv_inmat = temp_inv_inmat

        if IGNORE_3DMM_CODE:
            batch_size = self.base_c2w_Rmat.size(0)
            self.base_c2w_Tvec = torch.zeros_like(self.base_c2w_Tvec)
            self.base_c2w_Rmat = torch.eye(3).repeat(batch_size,1).view(batch_size,3,3)

        self.cam_info = {
            "batch_Rmats": self.base_c2w_Rmat.to(self.device),
            "batch_Tvecs": self.base_c2w_Tvec.to(self.device),
            "batch_inv_inmats": self.temp_inv_inmat.to(self.device).float()
        }
        

    @staticmethod
    def eulurangle2Rmat(angles):
        batch_size = angles.size(0)
        
        sinx = torch.sin(angles[:, 0])
        siny = torch.sin(angles[:, 1])
        sinz = torch.sin(angles[:, 2])
        cosx = torch.cos(angles[:, 0])
        cosy = torch.cos(angles[:, 1])
        cosz = torch.cos(angles[:, 2])

        rotXs = torch.eye(3, device=angles.device).view(1, 3, 3).repeat(batch_size, 1, 1)
        rotYs = rotXs.clone()
        rotZs = rotXs.clone()
        
        rotXs[:, 1, 1] = cosx
        rotXs[:, 1, 2] = -sinx
        rotXs[:, 2, 1] = sinx
        rotXs[:, 2, 2] = cosx
        
        rotYs[:, 0, 0] = cosy
        rotYs[:, 0, 2] = siny
        rotYs[:, 2, 0] = -siny
        rotYs[:, 2, 2] = cosy

        rotZs[:, 0, 0] = cosz
        rotZs[:, 0, 1] = -sinz
        rotZs[:, 1, 0] = sinz
        rotZs[:, 1, 1] = cosz
        
        res = rotZs.bmm(rotYs.bmm(rotXs))
        return res
    
    
    def build_code_and_cam(self):
        
        # code
        if self.include_eye_gaze:
            shape_code = torch.cat([self.base_iden + self.iden_offset, self.base_expr + self.expr_offset,self.base_gaze + self.gaze_offset], dim=-1)
            #appea_code = torch.cat([self.base_text, self.base_illu, self.base_gaze], dim=-1) + self.appea_offset
            appea_code = torch.cat([self.base_text, self.base_illu], dim=-1) + self.appea_offset
        else:
            shape_code = torch.cat([self.base_iden + self.iden_offset, self.base_expr + self.expr_offset], dim=-1)
            appea_code = torch.cat([self.base_text, self.base_illu], dim=-1) + self.appea_offset
        
        opt_code_dict = {
            "bg":None,
            "iden":self.iden_offset,
            "expr":self.expr_offset,
            "appea":self.appea_offset
        }

        if self.include_eye_gaze:
            opt_code_dict['gaze']=self.gaze_offset
        
        code_info = {
            "bg_code": None, 
            "shape_code":shape_code.float(), 
            "appea_code":appea_code.float(), 
        }

        #cam
        if self.opt_cam:
            delta_cam_info = {
                "delta_eulur": self.delta_EulurAngles, 
                "delta_tvec": self.delta_Tvecs
            }

            batch_delta_Rmats = self.eulurangle2Rmat(self.delta_EulurAngles)
            base_Rmats = self.cam_info["batch_Rmats"]
            base_Tvecs = self.cam_info["batch_Tvecs"]
            
            cur_Rmats = batch_delta_Rmats.bmm(base_Rmats)
            cur_Tvecs = batch_delta_Rmats.bmm(base_Tvecs) + self.delta_Tvecs
            
            batch_inv_inmat = self.cam_info["batch_inv_inmats"] #[N, 3, 3]    
            batch_cam_info = {
                "batch_Rmats": cur_Rmats,
                "batch_Tvecs": cur_Tvecs,
                "batch_inv_inmats": batch_inv_inmat
            }
            
        else:
            delta_cam_info = None
            batch_cam_info = self.cam_info


        return code_info, opt_code_dict, batch_cam_info, delta_cam_info
    
    
    @staticmethod
    def enable_gradient(tensor_list):
        for ele in tensor_list:
            ele.requires_grad = True


    def perform_fitting(self):
        self.delta_EulurAngles = torch.zeros((1, 3), dtype=torch.float32).to(self.device)
        self.delta_Tvecs = torch.zeros((1, 3, 1), dtype=torch.float32).to(self.device)

        self.iden_offset = torch.zeros((1, 100), dtype=torch.float32).to(self.device)
        self.expr_offset = torch.zeros((1, 79), dtype=torch.float32).to(self.device)
        self.appea_offset = torch.zeros((1, 127), dtype=torch.float32).to(self.device)

        if self.include_eye_gaze:
            self.gaze_offset = torch.zeros((1, self.eye_gaze_dim), dtype=torch.float32).to(self.device)
            self.enable_gradient([self.gaze_offset])
            #self.appea_offset = torch.cat((self.appea_offset,torch.zeros(1,self.eye_gaze_dim,device = self.device)),dim=1)

        if self.opt_cam:
            self.enable_gradient(
                [self.iden_offset, self.expr_offset, self.appea_offset, self.delta_EulurAngles, self.delta_Tvecs]
            )
        else:
            self.enable_gradient(
                [self.iden_offset, self.expr_offset, self.appea_offset]
            )
        
        init_learn_rate = 0.01
        
        step_decay = 300
        iter_num = 1
        
        params_group = [
            {'params': [self.iden_offset], 'lr': init_learn_rate * 1.5},
            {'params': [self.expr_offset], 'lr': init_learn_rate * 1.5},
            {'params': [self.appea_offset], 'lr': init_learn_rate * 1.0},
        ]

        if self.include_eye_gaze:
            params_group.append({'params': [self.gaze_offset], 'lr': init_learn_rate * 1.0})
        
        if self.opt_cam:
            params_group += [
                {'params': [self.delta_EulurAngles], 'lr': init_learn_rate * 0.1},
                {'params': [self.delta_Tvecs], 'lr': init_learn_rate * 0.1},
            ]
            
        optimizer = torch.optim.Adam(params_group, betas=(0.9, 0.999))
        lr_func = lambda epoch: 0.1 ** (epoch / step_decay)
        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_func) #adaptive learning rate
        
        gt_img = (self.img_tensor[0].detach().cpu().permute(1, 2, 0).numpy()* 255).astype(np.uint8)
        
        
        loop_bar = tqdm(range(iter_num), leave=True)
        for iter_ in loop_bar:
            with torch.set_grad_enabled(True):
                code_info, opt_code_dict, cam_info, delta_cam_info = self.build_code_and_cam()
                pred_dict = self.net( "test", self.xy, self.uv,  **code_info, **cam_info)
                #input: xy: torch.Size([1, 2, 1024]),   uv:torch.Size([1, 1024, 2]) 
                #code info: appea: torch.Size([1, 127]), shape:torch.Size([1, 179])
                #cam info : batch_Rmats: torch.Size([1, 3, 3])  batch_Tvecs:torch.Size([1, 3, 1])   batch_inv_inmats:torch.Size([1, 3, 3])
                #pred_dict['coarse_dict'] -> dict_keys(['merge_img', 'bg_img']) -> torch.Size([1, 3, 512, 512])

                
                batch_loss_dict = self.loss_utils.calc_total_loss(
                    delta_cam_info=delta_cam_info, opt_code_dict=opt_code_dict, pred_dict=pred_dict, disp_pred_dict=None,
                    gt_rgb=self.img_tensor, mask_tensor=self.mask_tensor
                )

            optimizer.zero_grad()
            batch_loss_dict["total_loss"].backward()
            optimizer.step()
            scheduler.step()   
            loop_bar.set_description("Opt, Loss: %.6f  " % batch_loss_dict["head_loss"].item())          

            # coarse_fg_rgb = pred_dict["coarse_dict"]["merge_img"]
            # coarse_fg_rgb = (coarse_fg_rgb[0].detach().cpu().permute(1, 2, 0).numpy()* 255).astype(np.uint8)
            # cv2.imwrite("./temp_res/opt_imgs/img_%04d.png" % iter_, coarse_fg_rgb[:, :, ::-1])

        coarse_fg_rgb = pred_dict["coarse_dict"]["merge_img"]
        coarse_fg_rgb = (coarse_fg_rgb[0].detach().cpu().permute(1, 2, 0).numpy()* 255).astype(np.uint8)
        coarse_fg_rgb = cv2.cvtColor(coarse_fg_rgb, cv2.COLOR_BGR2RGB)
        res_img = np.concatenate([gt_img, coarse_fg_rgb], axis=1)
        
        self.res_img = res_img
        self.res_code_info = code_info
        self.res_cam_info = cam_info
        


    def save_res(self, base_name, save_root):
        
        # Generate Novel Views
        render_nv_res = self.render_utils.render_novel_views(self.net, self.res_code_info)
        NVRes_save_path = "%s/FittingResNovelView_%s.gif" % (save_root, base_name)
        imageio.mimsave(NVRes_save_path, render_nv_res, 'GIF', duration=self.duration)
        
        # Generate Rendered FittingRes
        img_save_path = "%s/FittingRes_%s.png" % (save_root, base_name)

        self.res_img = put_text_alignmentcenter(self.res_img, self.pred_img_size, "Input", (0,0,0), offset_x=0)
        self.res_img = put_text_alignmentcenter(self.res_img, self.pred_img_size, "Fitting", (0,0,0), offset_x=self.pred_img_size,)

        # self.res_img = cv2.putText(self.res_img, "Input", (110, 240), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0,0,0), 1)
        # self.res_img = cv2.putText(self.res_img, "Fitting", (360, 240), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0,0,0), 1)
        cv2.imwrite(img_save_path, self.res_img[:,:,::-1])
        
        self.tar_code_info ={}
        direction_list = [LOWER_RIGHT,UPPER_RIGHT,UPPER_LEFT,LOWER_LEFT,LOWER_RIGHT]
        morph_res_seq = []
        vec_results_seq = []
        for i in range(len(direction_list)-1):
            start = direction_list[i]
            end = direction_list[i+1]

            shape_code = self.res_code_info['shape_code'].clone().detach()
            appea_code = self.res_code_info['appea_code'].clone().detach()
            # shape_code[0,-self.eye_gaze_dim:] = -shape_code[0,-self.eye_gaze_dim:]
            # appea_code[0,-self.eye_gaze_dim:] = -appea_code[0,-self.eye_gaze_dim:]
            shape_code[0,-self.eye_gaze_dim:] = gaze_feat_tensor(self.eye_gaze_dim,self.scale_factor,end)
            #appea_code[0,-self.eye_gaze_dim:] = torch.ones(self.eye_gaze_dim)
            self.tar_code_info['shape_code'] = shape_code.clone().detach()
            self.tar_code_info['appea_code'] = appea_code.clone().detach()
            self.tar_code_info['bg_code'] = None
            shape_code[0,-self.eye_gaze_dim:] = gaze_feat_tensor(self.eye_gaze_dim,self.scale_factor,start)
            #appea_code[0,-self.eye_gaze_dim:] = -torch.ones(self.eye_gaze_dim)
            self.res_code_info['shape_code'] = shape_code.clone().detach()
            self.res_code_info['appea_code'] = appea_code.clone().detach()

            morph_res,vec_results = self.render_utils.render_gaze_redirect_res(self.net, self.res_code_info, self.tar_code_info, self.view_num,self.scale_factor,vis_vect=self.vis_vect)
            #morph_res = self.render_utils.render_morphing_res(self.net, self.res_code_info, self.tar_code_info, self.view_num)

            morph_res_seq += morph_res
            vec_results_seq += vec_results


        morph_save_path = "%s/FittingResMorphing_%s.gif" % (save_root, base_name)
        imageio.mimsave(morph_save_path, morph_res_seq, 'GIF', duration=self.duration)

        image_vec_path = "%s/image_seq/" % (save_root)
        if not os.path.exists(image_vec_path):
            os.mkdir(image_vec_path)
        for ind,img_vec in enumerate(vec_results_seq):
            cv2.imwrite(os.path.join(image_vec_path,f"image{ind}.png"),img_vec)

        for k, v in self.res_code_info.items():
            if isinstance(v, torch.Tensor):
                self.res_code_info[k] = v.detach()
        
        temp_dict = {
            "code": self.res_code_info
        }

        torch.save(temp_dict, "%s/LatentCodes_%s_%s.pth" % (save_root, base_name, self.model_name))


    def fitting_single_images(self, hdf_file_path,image_index, save_root):
        self.load_data(hdf_file_path,image_index)
        # base_name = os.path.basename(img_path)[4:-4]

        self.perform_fitting()
        self.save_res(f'processed_image{image_index}', save_root)
    


    def _display_current_rendered_image(self,pred_dict,img_tensor):
        coarse_fg_rgb = pred_dict["coarse_dict"]["merge_img"]
        coarse_fg_rgb = (coarse_fg_rgb[0].detach().cpu().permute(1, 2, 0).numpy()* 255).astype(np.uint8)
        gt_img = (img_tensor[0].detach().cpu().permute(1, 2, 0).numpy()* 255).astype(np.uint8)
        res_img = np.concatenate([gt_img, coarse_fg_rgb], axis=1)


        cv2.imshow('current rendering', res_img)
        cv2.waitKey(0) 
        #closing all open windows 
        cv2.destroyAllWindows() 

def str2bool(v):
    return v.lower() in ('true', '1')

if __name__ == "__main__":
    torch.manual_seed(45)  # cpu
    torch.cuda.manual_seed(55)  # gpu
    np.random.seed(65)  # numpy
    random.seed(75)  # random and transforms
    # torch.backends.cudnn.deterministic = True  # cudnn
    # torch.backends.cudnn.benchmark = True
    # torch.backends.cuda.matmul.allow_tf32 = False
    # torch.backends.cudnn.allow_tf32 = False

    parser = argparse.ArgumentParser(description='a framework for fitting a single image using HeadNeRF')
    parser.add_argument("--model_path", type=str, required=True)
    
    parser.add_argument("--hdf_file", type=str, required=True)
    parser.add_argument("--image_index", type=int, required=True)
    parser.add_argument("--gaze_dim", type=int, required=True)

    parser.add_argument("--save_root", type=str, required=True)

    parser.add_argument("--eye_gaze_scale_factor", type=int, default=1)
    parser.add_argument("--vis_gaze_vect", type=str2bool, required=True,help='whether to visualize the gaze vector in result images')
    
    args = parser.parse_args()


    model_path = args.model_path
    save_root = args.save_root
    
    hdf_file = args.hdf_file
    image_index = args.image_index
    gaze_feat_dim = args.gaze_dim
    scale_factor = args.eye_gaze_scale_factor
    vis_vect = args.vis_gaze_vect
    
    # if len(args.target_embedding) == 0:
    #     target_embedding_path = None
    # else:
    #     target_embedding_path = args.target_embedding
    
    #     temp_str_list = target_embedding_path.split("/")
    #     if temp_str_list[1] == "*":
    #         temp_str_list[1] = os.path.basename(model_path)[:-4]
    #         target_embedding_path = os.path.join(*temp_str_list)
        
    #     assert os.path.exists(target_embedding_path)
    
    tt = FittingImage(model_path, save_root, gpu_id=0,include_eye_gaze=True,eye_gaze_dim=gaze_feat_dim,gaze_scale_factor=scale_factor,vis_vect=vis_vect)
    tt.fitting_single_images(hdf_file,image_index, save_root)
